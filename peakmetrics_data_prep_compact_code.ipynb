{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertTokenizer\n",
    "from transformers import AutoModelForTokenClassification, DistilBertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from scipy import spatial\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132757, 12)\n",
      "(85665, 12)\n",
      "(2726, 10)\n"
     ]
    }
   ],
   "source": [
    "#Read in files; download from S3 handled via cmd line\n",
    "def read_files(path):\n",
    "    files_glob=glob.glob(path)\n",
    "    file_collection=[]\n",
    "    for f in files_glob:\n",
    "        file_collection.append(pd.read_parquet(f))\n",
    "    file_collection=pd.concat(file_collection,axis=0)\n",
    "    return file_collection\n",
    "\n",
    "path=os.getcwd()\n",
    "\n",
    "news_files=path+'/news/*.parquet'\n",
    "news=read_files(news_files)\n",
    "print(news.shape)\n",
    "\n",
    "social_files=path+'/social/*.parquet'\n",
    "social=read_files(social_files)\n",
    "print(social.shape)\n",
    "\n",
    "blog_files=path+'/blog/*.parquet'\n",
    "blog=read_files(blog_files)\n",
    "print(blog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not all records have text in their bodies but their headlines/titles can still hold useful info\n",
    "news['all_text']=news.apply(lambda x: f\"{str(x['headline'])}. {str(x['summary'])}. {str(x['body'])}\", axis=1)\n",
    "social['all_text']=social.apply(lambda x: f\"{str(x['title'])}. {str(x['text'])}\", axis=1)\n",
    "blog['all_text']=blog.apply(lambda x: f\"{str(x['title'])}. {str(x['body'])}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85665/85665 [00:00<00:00, 245007.39it/s]\n",
      "100%|██████████| 85665/85665 [00:00<00:00, 723690.41it/s]\n",
      "  0%|          | 0/85665 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 85665/85665 [00:24<00:00, 3487.46it/s]\n",
      "100%|██████████| 2726/2726 [00:00<00:00, 11071.86it/s]\n",
      "100%|██████████| 2726/2726 [00:00<00:00, 53430.62it/s]\n",
      "100%|██████████| 2726/2726 [00:22<00:00, 122.91it/s]\n"
     ]
    }
   ],
   "source": [
    "#get some information on text length\n",
    "def count_text_elements(df, text_col, tokenizer):\n",
    "    df['word_count']=df[text_col].progress_map(lambda x: len(x.split(' ')))\n",
    "    df['sent_count']=df[text_col].progress_map(lambda x: len(x.split('.')))\n",
    "    df['token_count']=df[text_col].progress_map(lambda x: len(tokenizer(x)['input_ids']))\n",
    "    return df\n",
    "\n",
    "mpnet_tokenizer=AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "news=count_text_elements(news, 'all_text', mpnet_tokenizer)\n",
    "social=count_text_elements(social, 'all_text', mpnet_tokenizer)\n",
    "blog=count_text_elements(blog, 'all_text', mpnet_tokenizer)\n",
    "\n",
    "minilm_tokenizer=AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', max_length=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leverage NER to identify records dealing with individual airlines and the industry in general\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy='simple', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/857 [00:08<11:35,  1.22it/s]C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 857/857 [11:08<00:00,  1.28it/s]\n",
      " 36%|███▌      | 10/28 [00:27<00:49,  2.75s/it]C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 28/28 [01:14<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "def ner_scan(texts_list, tokenizer, model, aggregation_strategy='simple', chunk_size=100, outfile=None, device=-1):\n",
    "    ner_results = []\n",
    "    chunk_size=chunk_size\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=aggregation_strategy, device=device)\n",
    "    for chunk in tqdm(range(len(texts_list) // chunk_size + 1), position=0, leave=True):\n",
    "        texts = texts_list[chunk_size * chunk: min(chunk_size * (chunk+1),len(texts_list))]\n",
    "        res = ner_pipeline(texts)\n",
    "        ner_results += res\n",
    "    if outfile != None:\n",
    "        with open(outfile,'wb') as f:\n",
    "            pickle.dump(ner_results, f)\n",
    "    return ner_results\n",
    "\n",
    "news_text=news['all_text'].to_list()\n",
    "news['ner_results']=ner_scan(news_text, ner_tokenizer, ner_model, device=0)\n",
    "news['entities']=news['ner_results'].map(lambda x: [(y['word'],y['entity_group'],y['score']) for y in x])\n",
    "news['orgs']=news['entities'].map(lambda x: [y[0] for y in x if y[1]=='ORG'])\n",
    "\n",
    "social_text=social['all_text'].to_list()\n",
    "social['ner_results']=ner_scan(social_text, ner_tokenizer, ner_model, device=0)\n",
    "social['entities']=social['ner_results'].map(lambda x: [(y['word'],y['entity_group'],y['score']) for y in x])\n",
    "social['orgs']=social['entities'].map(lambda x: [y[0] for y in x if y[1]=='ORG'])\n",
    "\n",
    "blog_text=blog['all_text'].to_list()\n",
    "blog['ner_results']=ner_scan(blog_text, ner_tokenizer, ner_model, device=0)\n",
    "blog['entities']=blog['ner_results'].map(lambda x: [(y['word'],y['entity_group'],y['score']) for y in x])\n",
    "blog['orgs']=blog['entities'].map(lambda x: [y[0] for y in x if y[1]=='ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [29:23<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#Embeddings for search \n",
    "#--- we are doing this in order to avoid having to compute embeddings for each record, one at a time during the search process \n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "news_orgs=list(itertools.chain.from_iterable(news['orgs'].to_list()))\n",
    "blog_orgs=list(itertools.chain.from_iterable(blog['orgs'].to_list()))\n",
    "social_orgs=list(itertools.chain.from_iterable(social['orgs'].to_list()))\n",
    "orgs=list(set(news_orgs+blog_orgs+social_orgs))\n",
    "org_embeddings={}\n",
    "chunk_size=100\n",
    "for chunk in tqdm(range(len(orgs) // chunk_size + 1), position=0, leave=True):\n",
    "    texts = orgs[chunk_size * chunk: min(chunk_size * (chunk+1),len(orgs))]\n",
    "    for org in texts:\n",
    "        org_embeddings[org]=embedding_model.encode(org)\n",
    "with open('org_embeddings.pickle','wb') as f:\n",
    "    pickle.dump(org_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2726/2726 [00:12<00:00, 224.12it/s]\n",
      "100%|██████████| 85665/85665 [02:36<00:00, 546.31it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "us_domestic_airlines=['united airlines',\n",
    "'southwest airlines',\n",
    "'american airlines',\n",
    "'spirit airlines',\n",
    "'delta air lines',\n",
    "'frontier airlines',\n",
    "'alaska airlines',\n",
    "'delta air lines',\n",
    "'hawaiian airlines',\n",
    "'jetblue airways',\n",
    "'allegiant air',\n",
    "'envoy air',\n",
    "'republic airways',\n",
    "'skywest airlines']\n",
    "us_domestic_airlines_embeddings=[embedding_model.encode(x) for x in us_domestic_airlines]\n",
    "\n",
    "with open('org_embeddings.pickle','rb') as f:\n",
    "    org_embeddings=pickle.load(f)\n",
    "\n",
    "#Run a comparison between all orgs identified by the NER search and curated list of domestic airline names\n",
    "#Leverage embedings to avoid pitfalls of substring searches\n",
    "def search_for_airlines(entities_list,airline_embeddings):\n",
    "    airline_embeddings=airline_embeddings\n",
    "    orgs=[x[0] for x in entities_list if x[1]=='ORG']\n",
    "    max_score=0\n",
    "    max_score_org=''\n",
    "    exact_matches=[]\n",
    "    for org in orgs:\n",
    "        org_embed=org_embeddings[org]\n",
    "        for i in range(len(airline_embeddings)):\n",
    "            score=util.pytorch_cos_sim(airline_embeddings[i], org_embed)[0][0].item()\n",
    "            if score>0.99:\n",
    "                exact_matches.append(airline_embeddings[i])\n",
    "            if score>max_score:\n",
    "                max_score_org=org\n",
    "                max_score=score\n",
    "    return (np.round(max_score,2), max_score_org, exact_matches)\n",
    "\n",
    "news['airline_search']=news['entities'].progress_map(lambda x: search_for_airlines(x, us_domestic_airlines_embeddings))\n",
    "news['airline_match_score']=news['airline_search'].map(lambda x: x[0])\n",
    "news['best_match_org']=news['airline_search'].map(lambda x: x[1])\n",
    "\n",
    "blog['airline_search']=blog['entities'].progress_map(lambda x: search_for_airlines(x, us_domestic_airlines_embeddings))\n",
    "blog['airline_match_score']=blog['airline_search'].map(lambda x: x[0])\n",
    "blog['best_match_org']=blog['airline_search'].map(lambda x: x[1])\n",
    "\n",
    "social['airline_search']=social['entities'].progress_map(lambda x: search_for_airlines(x, us_domestic_airlines_embeddings))\n",
    "social['airline_match_score']=social['airline_search'].map(lambda x: x[0])\n",
    "social['best_match_org']=social['airline_search'].map(lambda x: x[1])\n",
    "\n",
    "#Make identifier column for each US carrier to assist with later analysis and slicing\n",
    "for airline in us_domestic_airlines:\n",
    "   # news[airline]=news['airline_search'].map(lambda x: 1 if airline in x[1].lower() else 0)\n",
    "    blog[airline]=blog['airline_search'].map(lambda x: 1 if airline in x[1].lower() else 0)\n",
    "    social[airline]=social['airline_search'].map(lambda x: 1 if airline in x[1].lower() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132757, 35)\n",
      "(85665, 35)\n",
      "(2726, 33)\n"
     ]
    }
   ],
   "source": [
    "print(news.shape)\n",
    "print(social.shape)\n",
    "print(blog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data save checkpoint\n",
    "with open('news.pickle', 'wb') as f:\n",
    "    pickle.dump(news, f)\n",
    "with open('blog.pickle', 'wb') as f:\n",
    "    pickle.dump(blog, f)\n",
    "with open('social.pickle', 'wb') as f:\n",
    "    pickle.dump(social,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings for downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a smaller embedding space for quicker compute downstream\n",
    "minilm_embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "mpnet_embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16029/16029 [01:41<00:00, 158.14it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_15332\\3241747728.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_airlines['minilm_embeddings']=news_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
      "100%|██████████| 16029/16029 [03:52<00:00, 68.98it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_15332\\3241747728.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_airlines['mpnet_embeddings']=news_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16029, 37)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out non-airline related records and sources\n",
    "exclude_news_source=['MDPI',#peer-reviewd journal\n",
    "                     'Nature',#peer-reviewd journal\n",
    "                     'Moviebill',#? movie time listing? but the articles don't seem to line up\n",
    "                     'legacy.com',#obituaries website\n",
    "                     'thirstyhorseway.biz',#?\n",
    "                     'hotnigerianjobs.com',#job board\n",
    "                     'members.avjobs.com',#job board\n",
    "                     'eBay'#ebay model airplane listings and such\n",
    "                    ]\n",
    "news_airlines=news[news.apply(lambda x: x['language']=='en' and x['airline_match_score']>0.75\n",
    "                              and x['source'] not in exclude_news_source, axis=1)]\n",
    "news_airlines['minilm_embeddings']=news_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "news_airlines['mpnet_embeddings']=news_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n",
    "news_airlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:03<00:00, 67.24it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\2566203590.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  blog_airlines['mpnet_embeddings']=blog_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n",
      "100%|██████████| 234/234 [00:01<00:00, 181.52it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\2566203590.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  blog_airlines['minilm_embeddings']=blog_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(234, 35)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_airlines=blog[blog.apply(lambda x: x['language']=='en' and x['airline_match_score']>0.75, axis=1)]\n",
    "blog_airlines['mpnet_embeddings']=blog_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n",
    "blog_airlines['minilm_embeddings']=blog_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "blog_airlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23314/23314 [03:26<00:00, 112.72it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\154352193.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  social_airlines['mpnet_embeddings']=social_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n",
      "100%|██████████| 23314/23314 [01:36<00:00, 242.83it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\154352193.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  social_airlines['minilm_embeddings']=social_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23314, 37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_social=['laxradar - Twitter',#automated twitter account, not valuable for analysis\n",
    "                'sarpy_spotter',#automated twitter account,\n",
    "                'centralspotter',#automated twitter account\n",
    "                'AboveStLouis',#automated twitter account\n",
    "                'RI_Aircraft',#automated twitter account\n",
    "                'LHRFlightBot',#automated twitter account\n",
    "                'skyoverhavant',#automated twitter account\n",
    "                'whats_above_SE1',#automated twitter account\n",
    "                'abovestockport'#automated twitter account\n",
    "                'LAS Runways - YouTube']\n",
    "\n",
    "social_airlines=social[social.apply(lambda x: x['language']=='en' and x['airline_match_score']>0.75\n",
    "                                    and x['source'] not in exclude_social, axis=1)]\n",
    "social_airlines['mpnet_embeddings']=social_airlines.progress_apply(lambda x: mpnet_embedding_model.encode(x['all_text']), axis=1)\n",
    "social_airlines['minilm_embeddings']=social_airlines.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "social_airlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85665/85665 [06:01<00:00, 237.11it/s]\n",
      "100%|██████████| 2726/2726 [00:38<00:00, 70.65it/s] \n"
     ]
    }
   ],
   "source": [
    "#news['minilm_embeddings']=news.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "social['minilm_embeddings']=social.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "blog['minilm_embeddings']=blog.progress_apply(lambda x: minilm_embedding_model.encode(x['all_text']), axis=1)\n",
    "data save checkpoint\n",
    "with open('news.pickle', 'wb') as f:\n",
    "    pickle.dump(news, f)\n",
    "with open('blog.pickle', 'wb') as f:\n",
    "    pickle.dump('blog', f)\n",
    "with open('social.pickle', 'wb') as f:\n",
    "    pickle.dump('social.pickle',f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model='cardiffnlp/twitter-roberta-base-sentiment-latest', tokenizer=sent_tokenizer,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23314 [00:00<?, ?it/s]C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "  0%|          | 7/23314 [00:00<06:11, 62.80it/s]C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 23314/23314 [02:17<00:00, 170.05it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\614891716.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  social_airlines['sentiment']=social_airlines['all_text'].progress_map(lambda x: sentiment_task(x,return_all_scores=True,**tokenizer_kwargs))\n",
      "  0%|          | 0/234 [00:00<?, ?it/s]C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\habu8\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 234/234 [00:02<00:00, 91.12it/s]\n",
      "C:\\Users\\habu8\\AppData\\Local\\Temp\\ipykernel_3316\\614891716.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  blog_airlines['sentiment']=blog_airlines['all_text'].progress_map(lambda x: sentiment_task(x,return_all_scores=True,**tokenizer_kwargs))\n"
     ]
    }
   ],
   "source": [
    "#news_airlines['sentiment']=news_airlines['all_text'].progress_map(lambda x: sentiment_task(x,return_all_scores=True,**tokenizer_kwargs))\n",
    "social_airlines['sentiment']=social_airlines['all_text'].progress_map(lambda x: sentiment_task(x,return_all_scores=True,**tokenizer_kwargs))\n",
    "blog_airlines['sentiment']=blog_airlines['all_text'].progress_map(lambda x: sentiment_task(x,return_all_scores=True,**tokenizer_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news.pickle', 'wb') as f:\n",
    "    pickle.dump(news, f)\n",
    "with open('blog.pickle', 'wb') as f:\n",
    "    pickle.dump(blog, f)\n",
    "with open('social.pickle', 'wb') as f:\n",
    "    pickle.dump(social,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
